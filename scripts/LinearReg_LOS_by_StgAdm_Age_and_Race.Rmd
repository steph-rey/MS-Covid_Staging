---
title: "Linear Regression Analysis -- Can LOS be explained by admission stage, age, and race?"
author: "Steph Reynolds (Stephanie.Reynolds@ucsf.edu)"
date: "`r format(Sys.time(), '%b %d, %Y  %H:%M %p')`"
output:
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Background

## Project 
MINDSCAPE: Modeling of infectious network dynamics for surveillance, control and prevention enhancement

## Description
This file imports `dm_stg_unique_pt_11.08.21.csv` and generates a linear regression model to evaluate the extent to which length of stay can be explained by admission stage, age, and race. Model parameters include: 

  * `LOS` -> Length of stay that patient was hospitalized (in days)
    - 4 different regression models where `LOS` is: (1) not transformed, (2) log-transformed, (3) square root-transformed, and (4) cube root-transformed
  * `stage_adm_cat` -> Admission stage categorized into moderate (stages 4-5) and severe (stages 6+)
  * `age` -> Age of patient at admission
  * `race` -> Race category (Black, White, Asian, Haw/PI, Other, Unknown)
    **Still unsure how to treat `race`**

## Important Notes 

  * Decided a-priori that we would include stage of admission, age, and race.
  * Used fastDummies::dummy_cols to create dummy columns for race and age, then use polspline::polymars(y,x) to identify which parameters were most important. Confirmed that we should include admission stage, age, and race as predictors. It also showed us that we need to include age as a non-linear in our model - relationship changed as age increased linearly. 
  * Included age as a polynomial to 2nd degree in regression formula. Tried out poly(age, 3) and poly(age, 4) too, but resulting p-values were less significant. 
  * Try different transformations of outcome (LOS): Log, square root, cubic root.
  * Applied mgcv::gam with spline or smooth function (s(age)), which confirmed that we should use age as polynomial.
  * As of now, undecided on how to treat or parameterize race. Regression results show significance for Race-Other category, and we are not sure why this could be. 

# Load required packages
```{r Load required packages, include=F}
library(tidyverse)
library(tidymodels)
library(performance) # check model assumptions and performance
library(vip) # calculate and visualize feature importance 
```

# Import and preview data 
```{r Import and preview data}
df <- readRDS("/Users/sreynolds2/Documents/GitHub/MS-Covid_Staging/data/covid_dm_stages_unique_FINAL_4.14.22.rds")

head(df, n=10)
```

# Create dataset with only relevant parameters and assign to `regdf`
```{r Prepare dataset for linear regression}
regdf <- df %>% 
  select(ID, age, sex, race, ethnicity, LOS, last_code_status, stage_adm, stage_adm.cat)
```

# Create dataset that excludes LOS outliers
**ASK TRAVIS WHETHER TO RUN REGRESSIONS WITH FULL DATASET OR WITHOUT LOS OUTLIERS**
**CAN DELETE THIS SECTION IF NOT USING DATASET WITHOUT OUTLIERS** 
**CAN ALSO USE MASS::RLM() FIT LINEAR MODEL BY ROBUST REGRESSION USING M ESTIMATOR**
```{r Create dataset that excludes LOS outliers, eval = FALSE}
# Identify outliers using boxplot method; save to vector
outliers <- boxplot(regdf$LOS)$out
  
# Exclude outliers from dataset
regdf_no_outliers <- regdf[-which(regdf$LOS %in% outliers), ]
```
# Fit linear regression model m.full
LOS ~ stage_adm.cat + poly(age, 2) + race + age + sex + ethnicity + last_code_status
where LOS is not transformed
```{r Linear regression model m1}
# Fit model
m.full <- lm(LOS ~ stage_adm.cat + poly(age, 2) + race + age + sex + ethnicity + last_code_status, data = regdf)

# Summarize regression output
summary(m.full)

# Check assumptions for `m1`
check_model(m.full)

# Check performance for `m1`
model_performance(m.full)
```

Age, race, and ethnicity appear to not be significant. Remove from model and re-run.

# Fit linear regression model m1
LOS ~ stage_adm.cat + poly(age, 2) + sex + race + last_code_status
where LOS is not transformed
```{r Linear regression model m1}
# Fit model
m1 <- lm(LOS ~ stage_adm.cat + poly(age, 2) + sex + race + last_code_status, data = regdf)

# Summarize regression output
summary(m1)

# Check assumptions for `m1`
check_model(m1)

# Check performance for `m1`
model_performance(m1)
```

# Fit linear regression model m2
LOS ~ stage_adm.cat + poly(age, 2) + sex + race + last_code_status
where LOS is transformed on log scale 
```{r Linear regression model m2}
# Fit model
m2 <- lm(log(LOS) ~ stage_adm.cat + poly(age, 2) + sex + race + last_code_status, data = regdf)

# Summarize regression output
summary(m2)

# Check assumptions for `m2`
check_model(m2)

# Check performance for `m2`
model_performance(m2)
```

Sex not found to be significant. Remove from model and re-run. 

# Fit linear regression model m3
LOS ~ stage_adm.cat + poly(age, 2) + race + last_code_status
where LOS is transformed on log scale 
```{r Linear regression model m2}
# Fit model
m3 <- lm(log(LOS) ~ stage_adm.cat + poly(age, 2) + race + last_code_status, data = regdf)

# Summarize regression output
summary(m3)

# Check assumptions for `m3`
check_model(m3)

# Check performance for `m3`
model_performance(m3)
```

# Visualize most important features for `m1`
```{r Visualize most important features for `m1`}
# Visualize feature importance
m1 %>% vip(num_features = 15,
           geom = "point",
           aes = list(size = 3, color = '18bc9c')) + 
  theme_minimal(base_size = 14) +
  labs(title = "Linear Regression: Feature Importance")
  # Feature with highest importance is: stage_adm.cat

# Side-by-side boxplot of LOS using admission severity as key predictor 
regdf %>% ggplot(aes(LOS, stage_adm.cat)) +
  geom_boxplot() + 
  geom_jitter(alpha = 0.3) + 
  theme_minimal(base_size = 15) +
  labs(title = "Comparison of Predicted LOS\nusing Admission Stage as Key Predictor")

# Side-by-side boxplot of LOS using admission severity as key predictor 
# Using dataset WITHOUT outliers 
regdf_no_outliers %>% ggplot(aes(LOS, stage_adm.cat)) +
  geom_boxplot() + 
  geom_jitter(alpha = 0.3) + 
  theme_minimal(base_size = 15) +
  labs(title = "Comparison of Predicted LOS\nusing Admission Stage as Key Predictor")
```

# Visualize most important features for m2
```{r Visualize most important features for m2}
m2 %>% vip(num_features = 15,
           geom = "point",
           aes = list(size = 3, color = '18bc9c')) + 
  theme_minimal(base_size = 14) +
  labs(title = "Linear Regression: Feature Importance")
  # Feature with highest importance is: stage_adm.cat 
```

# Visualize most important features for m3
```{r Visualize most important features for m3}
m3 %>% vip(num_features = 15,
           geom = "point",
           aes = list(size = 3, color = '18bc9c')) + 
  theme_minimal(base_size = 14) +
  labs(title = "Linear Regression: Feature Importance")
  # Feature with highest importance is: stage_adm.cat
```
