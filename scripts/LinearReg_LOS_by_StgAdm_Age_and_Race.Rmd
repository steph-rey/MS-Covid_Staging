---
title: "Linear Regression Analysis -- Can LOS be explained by admission stage, age, and race?"
author: "Steph Reynolds (Stephanie.Reynolds@ucsf.edu)"
date: "`r format(Sys.time(), '%b %d, %Y  %H:%M %p')`"
output:
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Background

## Project 
MINDSCAPE: Modeling of infectious network dynamics for surveillance, control and prevention enhancement

## Description
This file imports `dm_stg_unique_pt_11.08.21.csv` and generates a linear regression model to evaluate the extent to which length of stay can be explained by admission stage, age, and race. Model parameters include: 

  * `LOS` -> Length of stay that patient was hospitalized (in days)
    - 4 different regression models where `LOS` is: (1) not transformed, (2) log-transformed, (3) square root-transformed, and (4) cube root-transformed
  * `stage_adm_cat` -> Admission stage categorized into moderate (stages 4-5) and severe (stages 6+)
  * `age` -> Age of patient at admission
  * `race` -> Race category (Black, White, Asian, Haw/PI, Other, Unknown)
    **Still unsure how to treat `race`**

## Important Notes 

  * Decided a-priori that we would include stage of admission, age, and race.
  * Used fastDummies::dummy_cols to create dummy columns for race and age, then use polspline::polymars(y,x) to identify which parameters were most important. Confirmed that we should include admission stage, age, and race as predictors. It also showed us that we need to include age as a non-linear in our model - relationship changed as age increased linearly. 
  * Included age as a polynomial to 2nd degree in regression formula. Tried out poly(age, 3) and poly(age, 4) too, but resulting p-values were less significant. 
  * Try different transformations of outcome (LOS): Log, square root, cubic root.
  * Applied mgcv::gam with spline or smooth function (s(age)), which confirmed that we should use age as polynomial.
  * As of now, undecided on how to treat or parameterize race. Regression results show significance for Race-Other category, and we are not sure why this could be. 

# Load required packages
```{r Load required packages, include=F}
library(tidyverse)
library(tidymodels)
library(performance) # check model assumptions and performance
library(vip) # calculate and visualize feature importance 
```

# Import and preview data 
```{r Import and preview data}
df <- read_csv("/Users/sreynolds2/Documents/GitHub/MS-Covid_Staging/data/dm_stg_unique_pt_11.08.21.csv")

head(df, n=10)
```

# Convert variables to factor type and create dataset with only relevant parameters
```{r Prepare dataset for linear regression}
# Assign list of variables to be converted to factor type 
vars_fct <- c("sex", "race", "ethnicity", "smoking", "death", "max_stage", "stage_adm", "stage_disc")

# Convert these variables to factor type
df <- df %>% 
  mutate_at(vars_fct, factor)

# Categorize admission stage into 'moderate' and 'severe'
# Create dataset with only relevant parameters and assign to `regdf`
regdf <- df %>% 
  mutate(stage_adm_cat = ifelse(stage_adm %in% 4:5, "Moderate", "Severe")) %>% 
  select(ID, age, race, LOS, stage_adm, stage_adm_cat)
```

# Create dataset that excludes LOS outliers
**ASK TRAVIS WHETHER TO RUN REGRESSIONS WITH FULL DATASET OR WITHOUT LOS OUTLIERS**
**CAN DELETE THIS SECTION IF NOT USING DATASET WITHOUT OUTLIERS** 
**CAN ALSO USE MASS::RLM() FIT LINEAR MODEL BY ROBUST REGRESSION USING M ESTIMATOR**
```{r Create dataset that excludes LOS outliers, eval = FALSE}
# Identify outliers using boxplot method; save to vector
outliers <- boxplot(regdf$LOS)$out
  
# Exclude outliers from dataset
regdf_no_outliers <- regdf[-which(regdf$LOS %in% outliers), ]
```

# Fit linear regression model m1: LOS ~ stage_adm_cat + poly(age, 2)
Where outcome (LOS) is not transformed
```{r Linear regression model m1}
# Fit model
m1 <- lm(LOS ~ stage_adm_cat + poly(age, 2) + race, data = regdf)

# Summarize regression output
summary(m1)

# Check assumptions for `m1`
check_model(m1)

# Check performance for `m1`
model_performance(m1)
```

# Fit linear regression model m2: log(LOS) ~ stage_adm_cat + poly(age, 2)
Where outcome (LOS) is transformed on log scale 
```{r Linear regression model m2}
# Fit model
m2 <- lm(log(LOS) ~ stage_adm_cat + poly(age, 2) + race, data = regdf)

# Summarize regression output
summary(m2)

# Check assumptions for `m2`
check_model(m2)

# Check performance for `m2`
model_performance(m2)
```

# Fit linear regression model m3: sqrt(LOS) ~ stage_adm_cat + poly(age, 2)
Where outcome (LOS) is transformed on square root scale
```{r Linear regression model m3}
# Fit model
m3 <- lm(sqrt(LOS) ~ stage_adm_cat + poly(age, 2) + race, data = regdf)

# Summarize regression output
summary(m3)

# Check assumptions for `m3`
check_model(m3)

# Check performance for `m3`
model_performance(m3)
```

# Fit linear regression model m4: (LOS^(1/3)) ~ stage_adm_cat + poly(age, 2)
Where outcome (LOS) is transformed on cube root scale 
```{r Linear regression model m4}
# Fit model
m4 <- lm(LOS^(1/3) ~ stage_adm_cat + poly(age, 2) + race, data = regdf)

# Summarize regression output
summary(m4)

# Check assumptions for `m2`
# check_model(m4) -- GENERATES ERROR MESSAGE 

# Check performance for `m2`
model_performance(m4)
```

# Visualize most important features for `m1`
```{r Visualize most important features for `m1`}
m1 %>% vip(num_features = 15,
           geom = "point",
           aes = list(size = 3, color = '18bc9c')) + 
  theme_minimal(base_size = 14) +
  labs(title = "Logistic Regression: Feature Importance")
  # Feature with highest importance are: stage_adm_cat == SEVERE

regdf %>% ggplot(aes(LOS, stage_adm_cat)) +
  geom_boxplot() + 
  geom_jitter(alpha = 0.3) + 
  theme_minimal(base_size = 15) +
  labs(title = "Comparison of Predicted LOS\nusing Admission Stage as Key Predictor")
```

# Visualize most important features for m2
```{r Visualize most important features for m2}
m2 %>% vip(num_features = 15,
           geom = "point",
           aes = list(size = 3, color = '18bc9c')) + 
  theme_minimal(base_size = 14) +
  labs(title = "Logistic Regression: Feature Importance")
  # Feature with highest importance are: stage_adm_cat == SEVERE
```

# Visualize most important features for m3
```{r Visualize most important features for m3}
m3 %>% vip(num_features = 15,
           geom = "point",
           aes = list(size = 3, color = '18bc9c')) + 
  theme_minimal(base_size = 14) +
  labs(title = "Logistic Regression: Feature Importance")
  # Feature with highest importance are: stage_adm_cat == SEVERE
```

# Visualize most important features for m4
```{r Visualize most important features for m4}
m4 %>% vip(num_features = 15,
           geom = "point",
           aes = list(size = 3, color = '18bc9c')) + 
  theme_minimal(base_size = 14) +
  labs(title = "Logistic Regression: Feature Importance")
  # Feature with highest importance are: stage_adm_cat == SEVERE
```

# End of Document

